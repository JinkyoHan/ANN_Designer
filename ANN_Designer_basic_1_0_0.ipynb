{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) 2023 Jinkyo Han\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ],
      "metadata": {
        "id": "-v-46FRH_ZYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "            ######################\n",
        "            #    ANN Designer    #\n",
        "            #     ver 1.0.0      #\n",
        "            ######################\n",
        "\n",
        "# Written by Jinkyo Han, OST, SNU NAOE\n",
        "# 38jinkyo@snu.ac.kr\n",
        "# Supervised by Do Kyun Kim, OST, SNU NAOE\n",
        "\n",
        "# Input:\n",
        "    # 1) your dataset\n",
        "    # 2) parameters for optimization: search space, threshold, n_calls, ...\n",
        "# Output:\n",
        "    # 1) hyperparameters-model performance set, by csv file\n",
        "    # 2) full-trained optimal model by '.h5'\n",
        "    # 3) convergence plot for BOwGP optimization\n",
        "\n",
        "# Algorithm\n",
        "    # Search Space : manually defined\n",
        "    # Search Strategy : BOwGP - \"n_calls\" should be adjusted as needed\n",
        "    # Estimation : Early Stopping, by nu-SVR, at 25% learning curve - \"threshold\" should be adjusted as needed\n",
        "        # ref: Baker et al. (2017), Accelerationg NAS using performance prediction.\n",
        "        # NuSVR would be pretrained, with \"n_samples\" of dataset.\n",
        "            # Learning Curve at 25% epochs - Final val_loss\n",
        "    # Early Stopping : implemented by tensorflow.keras"
      ],
      "metadata": {
        "id": "1PN-m99y9ju3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install requirements"
      ],
      "metadata": {
        "id": "WfALxw3yANhT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UABmJMWD9aOx"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import basic packages"
      ],
      "metadata": {
        "id": "4qTzAZzwANIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Basics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "import json\n",
        "#-----------------------------------------------------------\n",
        "# Add the required libraries for Bayesian optimization\n",
        "import skopt\n",
        "    # Caution : please open your scikit-optimize package,\n",
        "    # and then replace every 'np.int' into 'int', in 'transformer.py'\n",
        "    # Anaconda\\Lib\\site-packages\\skopt\\space\\transformers.py\n",
        "from skopt import gp_minimize\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "#-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "yidew7M39mnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Apply GPU provided by Google Colaboratory.\n",
        "\n",
        "Subscribing Colab PRO+ would be recommanded, for minimizing running time.\n",
        "Thank you Google!"
      ],
      "metadata": {
        "id": "-l9u6mN8AX7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Does GPU Works?\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "#-----------------------------------------------------------\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "print(device_lib.list_local_devices())\n",
        "#-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "m2ko0xRv9qKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Import you Dataset\n",
        "\n",
        "Indices should be clearly defined. Modify the **\"num_of_input\"**\n",
        "\n",
        "Random state could be defined by user."
      ],
      "metadata": {
        "id": "mMW2QIKXA3xW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#-----------------------------------------------------------\n",
        "\n",
        "# Number of inputs\n",
        "num_of_input = 5\n",
        "num_of_output = 1\n",
        "\n",
        "file_name = 'MyInput.xlsx'\n",
        "data = pd.read_excel(file_name)\n",
        "data_clean = data.dropna()\n",
        "\n",
        "# create a scaler object\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data_clean)\n",
        "data_clean_normalized = scaler.transform(data_clean)\n",
        "\n",
        "data_clean_normalized = pd.DataFrame(data_clean_normalized)\n",
        "\n",
        "print(data_clean_normalized)\n",
        "\n",
        "X_clean = data_clean_normalized.iloc[:, :num_of_input].values\n",
        "y_clean = data_clean_normalized.iloc[:, num_of_input+num_of_output-1].values\n",
        "\n",
        "print(\"\\nShapes of enable datasets: \")\n",
        "print(\"{}\".format(data_clean.shape))\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_clean, y_clean, test_size=0.2, random_state=7)\n",
        "#-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "jv_kCNUA9sG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Basic functions\n",
        "\n",
        "Modify **create_model** for further complexities, if needed."
      ],
      "metadata": {
        "id": "2ucv5_8vCCyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate earlyStopping callback\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "#-----------------------------------------------------------\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
        "#-----------------------------------------------------------\n",
        "\n",
        "\n",
        "# Function : Create Model\n",
        "#-----------------------------------------------------------\n",
        "def create_model(layer_nodes, learning_rate, activation, dropout_rate, optimizer, loss_function):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(num_of_input,)))\n",
        "\n",
        "    for nodes in layer_nodes:\n",
        "        model.add(tf.keras.layers.Dense(nodes, activation=activation))\n",
        "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(num_of_output))\n",
        "\n",
        "    opt = None\n",
        "    if optimizer == 'adam':\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=loss_function, metrics=['mse'])\n",
        "    return model\n",
        "#-----------------------------------------------------------\n",
        "\n",
        "# Function: show plot\n",
        "#-----------------------------------------------------------\n",
        "def show_plot(params, val_mse, epoch_rate, predicted_performance, toshow=False):\n",
        "    if toshow==True:\n",
        "        plt.plot(np.arange(int(params['epochs']*epoch_rate)), val_mse, label='Actual')\n",
        "        plt.axhline(y=predicted_performance, color='r', linestyle='dashed', label='Predicted')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Validation MSE')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "#-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "x_YUNUuy9wDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Search Space\n",
        "\n",
        "The most basic settings:\n",
        "Clearly define the space as needed.\n",
        "\n",
        "The default settings are for \"basic feed-forward ANN model\"."
      ],
      "metadata": {
        "id": "5CIoJ25UDEXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the search space\n",
        "#-----------------------------------------------------------\n",
        "    # Define the hyperparameter search space\n",
        "    # Caution: this\n",
        "max_layers = 4\n",
        "layer_nodes_space = [Integer(16, 256, name=f'layer_{i+1}_nodes') for i in range(max_layers)]\n",
        "space = [\n",
        "    Integer(2, max_layers, name='number_of_layers'),\n",
        "    *layer_nodes_space,\n",
        "    Integer(16, 256, name='batch_size'),\n",
        "    Categorical([400], name='epochs'),\n",
        "    Real(1e-6, 1e-1, \"log-uniform\", name='learning_rate'),\n",
        "    Categorical(['relu','sigmoid','gelu'], name='activation'),\n",
        "    Categorical(['huber','mae','logcosh'], name='loss_function'),\n",
        "    Categorical(['adam','sgd','rmsprop'], name='optimizer'),\n",
        "    Real(0, 0.5, name='dropout_rate')\n",
        "]\n",
        "#-----------------------------------------------------------\n",
        "\n",
        "\n",
        "# Function: Print Hyperparameters\n",
        "#-----------------------------------------------------------\n",
        "def print_model(params, layer_nodes):\n",
        "    print(\"----------------------------------------\")\n",
        "    print(\"Model Specifications:\")\n",
        "    print(f\"Number of Layers: {params['number_of_layers']}\")\n",
        "    print(f\"Layer Nodes: {layer_nodes}\")\n",
        "    print(f\"Learning Rate: {params['learning_rate']}\")\n",
        "    print(f\"Activation Function: {params['activation']}\")\n",
        "    print(f\"Loss Function: {params['loss_function']}\")\n",
        "    print(f\"Optimizer: {params['optimizer']}\")\n",
        "    print(f\"Epochs: {params['epochs']}\")\n",
        "    print(f\"Dropout Rate: {params['dropout_rate']}\")\n",
        "    print(f\"Batch Size: {params['batch_size']}\")\n",
        "    print(\"----------------------------------------\")\n",
        "#-----------------------------------------------------------\n",
        "\n",
        "\n",
        "# Function: SVR Trainer\n",
        "from sklearn.svm import NuSVR\n",
        "from sklearn.svm import SVR\n",
        "from skopt.sampler import Lhs\n",
        "from joblib import dump\n",
        "#-----------------------------------------------------------\n",
        "# Function: Model Performance Estimation Strategy\n",
        "    # by nu-SVR, would predict model performance\n",
        "def predict_final_performance(val_loss, model=None, total_epochs=None):\n",
        "    x_pred = np.array(val_loss).reshape(1,-1)\n",
        "    prediction = model.predict(x_pred)\n",
        "    return prediction\n",
        "\n",
        "def pretrain_Estimator(X, y, model=None):\n",
        "    if model is None:\n",
        "        model = NuSVR(kernel='rbf', nu=0.5, gamma=0.1, C=100)\n",
        "    model.fit(X, y)\n",
        "    dump(model, 'estimator.joblib')\n",
        "    return model\n",
        "#-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "gMP0_ncM9uOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Train SVR\n",
        "\n",
        "For reasonable estimation of final validation loss of each model,\n",
        "\n",
        "\"n_samples\" of random models would be evaluated.\n",
        "\n",
        "After training, SVR could estimate the final val_loss of models\n",
        "\n",
        "with high accuracy, at 25% of learning curve."
      ],
      "metadata": {
        "id": "CkfPLu4RDioX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train SVR - from random samples\n",
        "    # models: 25% learning curve - final_val_loss\n",
        "#-----------------------------------------------------------\n",
        "    # Create a sampler\n",
        "sampler = Lhs(lhs_type=\"classic\", criterion=None)\n",
        "    # Sample \"n_samples\" points from the search space\n",
        "samples = sampler.generate(space, n_samples=100)\n",
        "    # Convert samples to list of dictionaries\n",
        "random_datasets = [{dim.name: sample[i] for i, dim in enumerate(space)} for sample in samples]\n",
        "    # Initialize the lists to store the results\n",
        "val_losses_25pct = []\n",
        "final_val_losses = []\n",
        "    # Train each model and record the results\n",
        "for params in random_datasets:\n",
        "        # Create and compile the model\n",
        "    number_of_layers = params['number_of_layers']\n",
        "    layer_nodes = [params[f'layer_{i+1}_nodes'] for i in range(number_of_layers)]\n",
        "\n",
        "    model = create_model(\n",
        "        layer_nodes=layer_nodes,\n",
        "        learning_rate=params['learning_rate'],\n",
        "        activation=params['activation'],\n",
        "        dropout_rate=params['dropout_rate'],\n",
        "        optimizer=params['optimizer'],\n",
        "        loss_function=params['loss_function']\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=params['optimizer'], loss=params['loss_function'])\n",
        "\n",
        "        # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=int(params['epochs']*0.25), batch_size=params['batch_size'],\n",
        "                        validation_data=(X_val, y_val), verbose=2)\n",
        "\n",
        "        # Record the 25% val_loss\n",
        "    val_losses_25pct.append(history.history['val_loss'])\n",
        "\n",
        "    # Continue training to 100%\n",
        "    history = model.fit(X_train, y_train, epochs=params['epochs'],\n",
        "                        batch_size=params['batch_size'],\n",
        "                        validation_data=(X_val, y_val), verbose=2,\n",
        "                        callbacks=[early_stopping],\n",
        "                        initial_epoch=int(params['epochs']*0.25))\n",
        "\n",
        "    # Record the final val_loss\n",
        "    final_val_losses.append(history.history['val_loss'][-1])\n",
        "\n",
        "# Train the NuSVR model\n",
        "Learning_Curve_train = np.array(val_losses_25pct)\n",
        "Model_Performance_train = np.array(final_val_losses)\n",
        "\n",
        "estimator = pretrain_Estimator(\n",
        "    Learning_Curve_train,\n",
        "    Model_Performance_train)\n",
        "#-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "dWqGgO8T-vGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Objective Function for Bayesian Optimization with Gaussian Process\n",
        "\n",
        "Thresholds could be modified, but modifying the epoch_rate would *not* be welcomed."
      ],
      "metadata": {
        "id": "QQnZGTkDEQuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function: Objective Function\n",
        "from joblib import load\n",
        "estimator = load('estimator.joblib')\n",
        "#-----------------------------------------------------------\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    # User Input for Performance estimation\n",
        "    threshold=0.00015\n",
        "    epoch_rate=0.25 # When the model to be estimated\n",
        "    #-------------------------------------------------------\n",
        "\n",
        "    number_of_layers = params['number_of_layers']\n",
        "    layer_nodes = [params[f'layer_{i+1}_nodes'] for i in range(number_of_layers)]\n",
        "\n",
        "    model = create_model(\n",
        "        layer_nodes=layer_nodes,\n",
        "        learning_rate=params['learning_rate'],\n",
        "        activation=params['activation'],\n",
        "        dropout_rate=params['dropout_rate'],\n",
        "        optimizer=params['optimizer'],\n",
        "        loss_function=params['loss_function']\n",
        "    )\n",
        "\n",
        "    # Print the model structure\n",
        "    print_model(params, layer_nodes)\n",
        "\n",
        "    val_mse = []\n",
        "    for epoch in range(params['epochs']):\n",
        "        history = model.fit(X_train, y_train, epochs=1,\n",
        "                            batch_size=params['batch_size'],\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            callbacks=[early_stopping],verbose=2)\n",
        "        val_mse.append(history.history['val_mse'][-1])\n",
        "\n",
        "        if epoch == int(params['epochs'] * epoch_rate-1):\n",
        "            predicted_performance = predict_final_performance(val_mse, estimator)\n",
        "            print(\"\\nPredicted Performance: {}\\n\".format(predicted_performance))\n",
        "\n",
        "            # Plot actual vs. predicted learning curve\n",
        "            #show_plot(params, val_mse, epoch_rate, predicted_performance, toshow=False)\n",
        "\n",
        "            if predicted_performance >= threshold:\n",
        "                break\n",
        "\n",
        "    return val_mse[-1]\n",
        "#-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "-9_Dxn9e-2yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. MAIN\n",
        "\n",
        "for n_calls of iterations, BOwGP will find out the optimal model.\n",
        "\n",
        "logs(without verboses) would be automatically generated in .csv format."
      ],
      "metadata": {
        "id": "4rNjY-WuE_s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN: Run Bayesian optimization\n",
        "import time\n",
        "#-----------------------------------------------------------\n",
        "    # n_calls : iterations for iptimization\n",
        "    # Recommand : n_calls might be large enough, dependent to Search Space\n",
        "n_calls = 200\n",
        "res_gp = gp_minimize(objective, dimensions=space, n_calls=n_calls, random_state=7, verbose=True)\n",
        "\n",
        "    # Extract and print the best parameters\n",
        "best_params = res_gp.x\n",
        "print(\"\\nBest parameters:\", best_params)\n",
        "print(\"Best MSE:\", res_gp.fun)\n",
        "\n",
        "    # Convert results to DataFrame and save as CSV\n",
        "results_df = pd.DataFrame(res_gp.x_iters, columns=[dimension.name for dimension in space])\n",
        "results_df['MSE'] = res_gp.func_vals\n",
        "\n",
        "    # Optionally, add a column to indicate which configurations are better than a certain threshold\n",
        "result_threshold = 0.00005  # adjust this value as needed\n",
        "results_df['is_better_than_threshold'] = results_df['MSE'] < result_threshold\n",
        "\n",
        "    # Save to CSV\n",
        "results_df.to_csv(\"NAS_Result_{}.csv\".format(\n",
        "    time.strftime('%Y_%m_%d_%H_%M', time.localtime(time.time()))\n",
        "    ), index=False)\n",
        "#-----------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "osIczdUt-5PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Export\n",
        "\n",
        "The best model would be exported as .h5 format."
      ],
      "metadata": {
        "id": "c9X6nxIgFSRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export best '.h5' file\n",
        "#-----------------------------------------------------------\n",
        "best_number_of_layers = best_params[0]\n",
        "best_layer_nodes = best_params[1:1+best_number_of_layers]\n",
        "best_batch_size = best_params[1+max_layers]\n",
        "best_epochs = best_params[2+max_layers]\n",
        "best_learning_rate = best_params[3+max_layers]\n",
        "best_activation = best_params[4+max_layers]\n",
        "best_loss_function = best_params[5+max_layers]\n",
        "best_optimizer = best_params[6+max_layers]\n",
        "best_dropout_rate = best_params[7+max_layers]\n",
        "\n",
        "best_model = create_model(\n",
        "    layer_nodes=best_layer_nodes,\n",
        "    learning_rate=best_learning_rate,\n",
        "    activation=best_activation,\n",
        "    dropout_rate=best_dropout_rate,\n",
        "    optimizer=best_optimizer,\n",
        "    loss_function=best_loss_function,\n",
        ")\n",
        "\n",
        "best_model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size)\n",
        "best_model.save(\"Best_Model_{}.h5\".format(\n",
        "    time.strftime('%Y_%m_%d_%H_%M', time.localtime(time.time()))\n",
        "    ))\n",
        "\n",
        "best_val_loss = min(best_model.history.history['val_loss'])\n",
        "print(f\"Best Validation Loss: {best_val_loss}\")\n",
        "#-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "8Z0WxjK_-9HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. further visualizations of convergence curve. enjoy!"
      ],
      "metadata": {
        "id": "klTsdFpAFanr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "#-----------------------------------------------------------\n",
        "from skopt.plots import plot_convergence, plot_objective, plot_evaluations\n",
        "\n",
        "# Convergence plot\n",
        "plot_convergence(res_gp)\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "# Objective plot (can be computationally intensive for large search spaces)\n",
        "plot_objective(res_gp)\n",
        "plt.show()\n",
        "\n",
        "# Evaluation plot\n",
        "plot_evaluations(res_gp)\n",
        "plt.show()\n",
        "'''\n",
        "#-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "QkTPspQB-_Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. To rewind the normalization."
      ],
      "metadata": {
        "id": "pom94g4oFgyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a scaler object\n",
        "y_scaler = MinMaxScaler()\n",
        "y_scaler.fit(data_clean.iloc[:,num_of_input].values.reshape(-1,1))\n",
        "\n",
        "# Inverse Transform\n",
        "def inverse_transform_y(y_normalized):\n",
        "    y_reshaped = y_normalized.reshape(-1, num_of_output)\n",
        "    return y_scaler.inverse_transform(y_reshaped)"
      ],
      "metadata": {
        "id": "UogiGLskGSsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Comparison between the dataset, and predicted values."
      ],
      "metadata": {
        "id": "H4adODqJHwkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate prediction\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "print(y_val)\n",
        "print(y_pred)\n",
        "\n",
        "# Rescale y : remind - those were previously normalized by MixMaxScaler\n",
        "y_pred_rescaled = inverse_transform_y(y_pred)/315\n",
        "y_val_rescaled = inverse_transform_y(y_val)/315\n",
        "\n",
        "# for plot\n",
        "min_axis = min(np.amin(y_pred_rescaled), np.amin(y_val_rescaled))\n",
        "max_axis = max(np.amax(y_pred_rescaled), np.amax(y_val_rescaled))\n",
        "\n",
        "print(y_val_rescaled)\n",
        "print(y_pred_rescaled)\n",
        "\n",
        "# R squared\n",
        "r2 = r2_score(y_val_rescaled, y_pred_rescaled)\n",
        "\n",
        "# COV\n",
        "cov = (np.std(y_pred_rescaled - y_val_rescaled) / np.mean(y_val_rescaled))\n",
        "\n",
        "# Show Plot: Data vs Predicted Value\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(y_val_rescaled, y_pred_rescaled, alpha=0.4)\n",
        "plt.plot([min_axis, max_axis], [min_axis, max_axis], color='red', linestyle='--')  # y=x line\n",
        "plt.title(f\"Data vs Predicted (R^2: {r2:.5f}, COV: {cov:.5f})\")\n",
        "plt.xlabel(\"Data\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VM_32TZ2GjzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All tasks were finished.\n",
        "\n",
        "Thank you for using!"
      ],
      "metadata": {
        "id": "iMJ-lpwPH2nU"
      }
    }
  ]
}